{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a325b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9b0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe06f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # color conversion\n",
    "    image.flags.writeable = False # image is no longer writeable\n",
    "    results = model.process(image) # make prediction\n",
    "    image.flags.writeable = True # image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # color conversion\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e03723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # draw right hand connections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645ecaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb4c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501cb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data\n",
    "no_sequences = 40\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e76b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(['angry', 'cat', 'cold', 'dog', 'good', 'happy', 'Hello', 'hot',\n",
    "       'hungry', 'Im full', 'me', 'no', 'sad', 'sorry', 'Thank you',\n",
    "       'tired', 'we', 'why', 'worry', 'you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3f2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f0cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('videos.pkl', 'rb') as tf:\n",
    "    videos = pickle.load(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba88ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.pkl', 'rb') as tf:\n",
    "    sequences = pickle.load(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c137bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9a449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, activation='sigmoid', input_shape=(sequence_length, 1662)))\n",
    "model.add(LSTM(64, return_sequences=True, activation='sigmoid'))\n",
    "model.add(LSTM(32, return_sequences=False, activation='sigmoid'))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03dd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('sixth_trial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7e4b1",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaae1c7",
   "metadata": {},
   "source": [
    "- you(80)\n",
    "- worry(c:2 s:97)\n",
    "- why(0.02)\n",
    "- we(96)\n",
    "- tired(c:0.1 s:97)\n",
    "- thank you(1)\n",
    "- sorry(95)\n",
    "- sad(0.00001)\n",
    "- no(0.03)\n",
    "- me(0.1)\n",
    "- Im full(43)\n",
    "- hungry(47)\n",
    "- hot(h:95 h:0.000001)\n",
    "- Hello(0.002)\n",
    "- happy(0.08)\n",
    "- good(0.1)\n",
    "- dog(97)\n",
    "- cold(95)\n",
    "- cat(h:60 h:70)\n",
    "- angry(0.006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4ec8d",
   "metadata": {},
   "source": [
    "# GOOD(11) - WIN!!!\n",
    "### dog cold  you we sorry cat worry hot tired hungry Im full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ec201",
   "metadata": {},
   "source": [
    "# BAD(9)\n",
    "### why thankyou sad no me hello happy good angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78f7d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "res = np.zeros((20)) \n",
    "threshold = 0.7\n",
    "action = \"dog\"\n",
    "\n",
    "cap = cv2.VideoCapture(cv2.CAP_DSHOW)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction Logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-40:]\n",
    "        \n",
    "        if len(sequence) == 40:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0),verbose=None)[0]\n",
    "#             print(actions[np.argmax(res)], res[np.argmax(res)])\n",
    "            \n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "#         cv2.putText(image, \"{} {}\".format(actions[np.argmax(res)], res[np.argmax(res)]*100), (3, 30),\n",
    "#                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"{} {}\".format(action, res[label_map[action]]*100), (3, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d6df77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 99, 106, 111],\n",
       "        [ 97, 104, 109],\n",
       "        [ 98, 105, 104],\n",
       "        ...,\n",
       "        [ 47,  51,  59],\n",
       "        [ 47,  49,  55],\n",
       "        [ 46,  48,  54]],\n",
       "\n",
       "       [[ 96, 102, 110],\n",
       "        [ 99, 105, 113],\n",
       "        [ 97, 106, 106],\n",
       "        ...,\n",
       "        [ 46,  50,  58],\n",
       "        [ 46,  48,  49],\n",
       "        [ 48,  50,  51]],\n",
       "\n",
       "       [[ 96, 107, 114],\n",
       "        [ 93, 104, 111],\n",
       "        [ 99, 105, 108],\n",
       "        ...,\n",
       "        [ 45,  48,  60],\n",
       "        [ 41,  42,  51],\n",
       "        [ 47,  48,  57]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 96,  98,  99],\n",
       "        [103, 105, 106],\n",
       "        [106, 120, 118],\n",
       "        ...,\n",
       "        [117, 119, 119],\n",
       "        [115, 117, 117],\n",
       "        [116, 118, 118]],\n",
       "\n",
       "       [[ 93,  94, 103],\n",
       "        [ 94,  95, 104],\n",
       "        [ 97, 106, 111],\n",
       "        ...,\n",
       "        [114, 116, 117],\n",
       "        [110, 112, 112],\n",
       "        [113, 115, 115]],\n",
       "\n",
       "       [[ 97,  99, 114],\n",
       "        [ 92,  94, 109],\n",
       "        [ 93, 101, 109],\n",
       "        ...,\n",
       "        [117, 119, 120],\n",
       "        [117, 118, 114],\n",
       "        [118, 119, 115]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
