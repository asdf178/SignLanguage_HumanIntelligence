{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71822ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bedfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d461744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c47d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6973a32",
   "metadata": {},
   "source": [
    "# 한 번만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0e85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4bd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c3ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a024e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5fa4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29e107",
   "metadata": {},
   "source": [
    "# 동작 이름 입력하고 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a9c9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data\n",
    "no_sequences =  40\n",
    "\n",
    "# Videos are going to be 40 frames in length\n",
    "sequence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "624a63a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n",
      "CAN'T CREATE DIR\n"
     ]
    }
   ],
   "source": [
    "# Actions that we try to detect\n",
    "action = 'dog'\n",
    "\n",
    "for sequence in range(no_sequences):\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "    except:\n",
    "        print(\"CAN'T CREATE DIR\") # 폴더가 이미 존재합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1ddc434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for sequence in range(no_sequences):\n",
    "        if not running:\n",
    "            # delete action folder\n",
    "            shutil.rmtree(os.path.join(DATA_PATH, action))\n",
    "            print(action, \": DELETED\")\n",
    "            break\n",
    "        # Loop through video length aka sequence length\n",
    "        for frame_num in range(sequence_length+1):\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image,results)\n",
    "\n",
    "            # NEW Apply wait logic\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(2000) # wait for 2 seconds\n",
    "                    \n",
    "            else:\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence+1), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                # Exports keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                running = False\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a494f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9dee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
